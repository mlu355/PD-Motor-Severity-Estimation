{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(file, true_labels, to_glob=False):\n",
    "    \"\"\"\n",
    "    Calculate and print metrics for given file and labels \n",
    "    Metrics: roc_auc, cohen's kappa, f1, recall, precision\n",
    "    :param file: file to calculate metrics for\n",
    "    :param true_labels: ground truth labels\n",
    "    :param to_glob: if True, calculate metrics for all subdirectories of file param\n",
    "    \n",
    "    \"\"\"\n",
    "    globbed = False\n",
    "    if to_glob:\n",
    "        run_names = glob.glob(os.path.join(file, '*results.json'))\n",
    "        globbed = True\n",
    "    else:\n",
    "        run_names = [file]\n",
    "        \n",
    "    for file in run_names:\n",
    "        pred_classes_, pred_probs_ = get_predictions(file)\n",
    "        num_videos = min(len(pred_classes_), len(true_labels))\n",
    "        labels = true_labels[:num_videos]\n",
    "        pred_classes = pred_classes_[:num_videos]\n",
    "        pred_probs = pred_probs_[:num_videos]\n",
    "        macro_avg_auc, roc_aucs = calculate_auc(pred_probs, labels)\n",
    "        roc_aucs = np.array(roc_aucs)\n",
    "        print(\"\\nfile\", file)\n",
    "        print(\"macro average roc_auc:\", sum(roc_aucs) / 4)\n",
    "        print(\"roc_auc per class:\", roc_aucs)\n",
    "        print(\"\\n\", classification_report(labels, pred_classes))\n",
    "        print(\"\\n\", confusion_matrix(labels, pred_classes))\n",
    "        \n",
    "def calculate_auc(pred_probs, labels, num_classes=4):\n",
    "    \"\"\"\n",
    "    Calculates AUC given a list of true labels and predicted class probabilities\n",
    "    Computes macro-average ROC curve and ROC area\n",
    "    :param pred_probs: predicted class probabilities\n",
    "    :param labels: prediction labels\n",
    "    :param num_classes: number of prediction classes\n",
    "    :return: predicted class for one video\n",
    "    \"\"\"\n",
    "    num_videos = len(pred_probs)\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(num_videos):\n",
    "        label = labels[i]\n",
    "        video_pred = pred_probs[i]\n",
    "        for clip in video_pred:\n",
    "            label_list.append(label)\n",
    "            pred_list.append(clip)        \n",
    "    pred_list = np.array(pred_list)\n",
    "    one_hot_true_labels = np.eye(num_classes)[label_list]\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(one_hot_true_labels[:, i], pred_list[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    roc_aucs = []\n",
    "    for score in roc_auc:\n",
    "        roc_aucs.append(roc_auc[score])\n",
    "    return sum(roc_aucs) / 4, roc_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file jobs/default/results.json\n",
      "macro average roc_auc: 0.827447540233394\n",
      "roc_auc per class: [0.94863137 0.85840494 0.67765027 0.82510358]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91         5\n",
      "           1       0.80      0.80      0.80         5\n",
      "           2       0.67      0.50      0.57         4\n",
      "           3       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.78        18\n",
      "   macro avg       0.76      0.76      0.76        18\n",
      "weighted avg       0.77      0.78      0.77        18\n",
      "\n",
      "\n",
      " [[5 0 0 0]\n",
      " [1 4 0 0]\n",
      " [0 1 2 1]\n",
      " [0 0 1 3]]\n"
     ]
    }
   ],
   "source": [
    "# Example of results for all classes (with real dataset)\n",
    "exam_df = pd.read_csv('data/gait_labels.csv')\n",
    "y = exam_df['Label'].tolist()\n",
    "calculate_metrics('jobs/default', y, to_glob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
